{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_DF=pd.read_csv('train.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2  What is the step by step guide to invest in sh...   \n",
       "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      5     6  How can I increase the speed of my internet co...   \n",
       "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAN occurence in Data\n",
    "#Question2 as 'NAN' in it\n",
    "\n",
    "#total number of NAN in each Column\n",
    "#train_DF.isnull().sum()\n",
    "\n",
    "#Index which have NAN Values\n",
    "#train_DF.isnull().any(1).nonzero()\n",
    "#train_DF.iloc[[105780, 201841]]\n",
    "\n",
    "#Drop The NAN Values\n",
    "train_DF.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note\n",
    "#No duplicates in ID\n",
    "\n",
    "#train_DF['id_duplicate']=train_DF.duplicated(['id'])\n",
    "#train_DF[train_DF['id_duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "#duplicates exist in qid1\n",
    "#qid is the question ID for question 1\n",
    "\n",
    "#train_DF['qid1_duplicate']=train_DF.duplicated(['qid1'])\n",
    "#train_DF[train_DF['qid1_duplicate']==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "#duplicates exist in qid2\n",
    "#qid is the question ID for question 2\n",
    "\n",
    "#train_DF['qid2_duplicate']=train_DF.duplicated(['qid2'])\n",
    "#train_DF[train_DF['qid2_duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "# Question can be duplicates between Question1 and question 2\n",
    "#same question can come up as Question 1 in one example and Question 2 in other\n",
    "#Total Number of Unique Questions:347345\n",
    "\n",
    "\n",
    "#qid1=train_DF['qid1']\n",
    "#qid2=train_DF['qid2']\n",
    "#qid=qid1+qid2\n",
    "\n",
    "#Total Number of Unique Questions\n",
    "#print(len(qid.unique()))\n",
    "\n",
    "#Total Number of unique Questions in Question 1 + Total Number of Unique Questions in Question 2\n",
    "#print(len(qid2.unique())+len(qid1.unique()))\n",
    "\n",
    "#Maximum times a question repeated in data:6\n",
    "#qid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Features\n",
    "- Number of characters\n",
    "- Number of words\n",
    "- 'End' Character in Questions(?vs.)\n",
    "- TFIDf and LSA(30 dimension)\n",
    "- LDA (check performance- 20 dimensions)\n",
    "- Question type- What, When , How, Why( more types have to be added)\n",
    "- Number of Capital Letters, Special Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_DF['question1'] = train_DF['question1'].astype(str)\n",
    "train_DF['question2'] = train_DF['question2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def character_count(a,b):\n",
    "    return len(a), len(b)\n",
    "\n",
    "train_DF['character_count_in_question1'],train_DF['character_count_in_question2']= zip(*train_DF.apply(lambda row: \n",
    "                                                                                                       character_count(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(a,b):\n",
    "    return len(nltk.tokenize.word_tokenize(a)), len(nltk.tokenize.word_tokenize(b))\n",
    "\n",
    "train_DF['word_count_in_question1'],train_DF['word_count_in_question2']= zip(*train_DF.apply(lambda row: \n",
    "                                                                                                       word_count(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Difference in Word and Character Counts\n",
    "\n",
    "#count as feature\n",
    "#Approaches:\n",
    "#count_difference<range => 0\n",
    "\n",
    "#train_DF['difference_in_word_count']=abs(train_DF['word_count_in_question1']-train_DF['word_count_in_question2'])\n",
    "#train_DF['difference_in_char_count']=abs(train_DF['character_count_in_question1']-train_DF['character_count_in_question2'])\n",
    "\n",
    "#train_DF_diplicates=train_DF[train_DF['is_duplicate']==1]\n",
    "\n",
    "#difference_in_word_count\n",
    "#label less than 5 has 1\n",
    "#train_DF_diplicates['difference_in_word_count'].value_counts()\n",
    "#train_DF['difference_in_word_count'].value_counts()\n",
    "\n",
    "#difference_in_character_count\n",
    "#Label less than 20 as 1\n",
    "#train_DF_diplicates['difference_in_char_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Last Character Check\n",
    "def last_character(a,b):\n",
    "    if a[-1]=='?':\n",
    "        if b[-1]=='?':\n",
    "            return 1,1\n",
    "        else:\n",
    "            return 1,0\n",
    "    else:\n",
    "        if b[-1]=='?':\n",
    "            return 0,1\n",
    "        else:\n",
    "            return 0,0\n",
    "\n",
    "train_DF['Last_Character_in_question1'],train_DF['last_Character_in_question2']= zip(*train_DF.apply(lambda row: last_character(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(train_DF['question1']+train_DF['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "f = open('tfidf_vectorizer.save', 'wb')\n",
    "cPickle.dump(tfidf_vectorizer, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_question_1 = tfidf_vectorizer.transform(train_DF['question1'])\n",
    "tfidf_question_2 = tfidf_vectorizer.transform(train_DF['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: 87010\n",
      "  Actual number of tfidf features: 87010\n",
      "\n",
      "Performing dimensionality reduction using LSA\n"
     ]
    }
   ],
   "source": [
    "print(\"  Actual number of tfidf features: %d\" % tfidf_question_1.get_shape()[1])\n",
    "print(\"  Actual number of tfidf features: %d\" % tfidf_question_2.get_shape()[1])\n",
    "print(\"\\nPerforming dimensionality reduction using LSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('truncatedsvd', TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)), ('normalizer', Normalizer(copy=False, norm='l2'))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd = TruncatedSVD(50)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "lsa.fit(tfidf_question_1+tfidf_question_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('lsa.save', 'wb')\n",
    "cPickle.dump(lsa, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_question1 = lsa.transform(tfidf_question_1)\n",
    "lsa_question2 = lsa.transform(tfidf_question_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for string in train_DF['question1']+train_DF['question2']:\n",
    "    texts.append(nltk.tokenize.word_tokenize(string))\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, chunksize=1000,num_topics=25, id2word = dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('lda.save', 'wb')\n",
    "cPickle.dump(ldamodel, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.save('lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "preprocessed_texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in train_DF['question1']+train_DF['question2']:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    preprocessed_texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "preprocessed_dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "preprocessed_corpus = [preprocessed_dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# generate LDA model\n",
    "preprocessed_ldamodel = gensim.models.ldamodel.LdaModel(preprocessed_corpus, chunksize=1000, num_topics=50, id2word = preprocessed_dictionary, passes=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('preprocessed_lda.save', 'wb')\n",
    "cPickle.dump(preprocessed_ldamodel, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_ldamodel.save('preprocessed_lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
