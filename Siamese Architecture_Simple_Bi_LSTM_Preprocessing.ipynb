{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from nltk import tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDVECTORS_DIR='wordvectors/'\n",
    "INPUT_DATA_DIR='inputs/'\n",
    "WEIGHTS_DIR='weights/'\n",
    "OUTPUT_DIR='output/'\n",
    "\n",
    "EMBEDDING_FILE=WORDVECTORS_DIR+'glove.6B.100d.txt'\n",
    "TRAIN_DATA_FILE=INPUT_DATA_DIR+'train.csv'\n",
    "TEST_DATA_FILE=INPUT_DATA_DIR+'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF=pd.read_csv(TRAIN_DATA_FILE,index_col='id')\n",
    "test_DF=pd.read_csv(TEST_DATA_FILE)\n",
    "train_DF.dropna(inplace=True)\n",
    "train_DF.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_DF['question1'] = train_DF['question1'].astype(str)\n",
    "train_DF['question2'] = train_DF['question2'].astype(str)\n",
    "test_DF['question1'] = test_DF['question1'].astype(str)\n",
    "test_DF['question2'] = test_DF['question2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_index[\"|\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text):\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    \n",
    "    text = re.sub(r\"it's\", \"it is \", text)\n",
    "    text = re.sub(r\"won't\", \"wont \", text)\n",
    "    text = re.sub(r\"who's\", \"who is \", text)\n",
    "    text = re.sub(r\"why's\", \"why is \", text)\n",
    "    text = re.sub(r\"how's\", \"how is \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"that's\", \"that is \", text)\n",
    "    \n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    \n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "for text1, text2,label in zip(train_DF['question1'],train_DF['question2'],train_DF['is_duplicate']):\n",
    "    texts_1.append(text_to_wordlist(text1))\n",
    "    texts_2.append(text_to_wordlist(text2))\n",
    "    labels.append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['test_id', 'question1', 'question2'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_1 = [] \n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "for text1, text2,test_id in zip(test_DF['question1'],test_DF['question2'],test_DF['test_id']):\n",
    "    test_texts_1.append(text_to_wordlist(text1))\n",
    "    test_texts_2.append(text_to_wordlist(text2))\n",
    "    test_ids.append(int(test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1+texts_2+test_texts_1+test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1=pad_sequences(sequences_1,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2=pad_sequences(sequences_2,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_1=pad_sequences(test_sequences_1,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2=pad_sequences(test_sequences_2,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_weights = np.random.random((nb_words, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_weights[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(train_DF))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=nb_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_weights],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lstm = 100\n",
    "num_dense = 50\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 100)       12050000                                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           80400                                        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 200)           800                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            10050                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 50)            200                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             51                                           \n",
      "====================================================================================================\n",
      "Total params: 12,141,501.0\n",
      "Trainable params: 91,001.0\n",
      "Non-trainable params: 12,050,500.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_input=Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "embedding_left=embedding_layer(left_input)\n",
    "left_output=lstm_layer(embedding_left)\n",
    "\n",
    "right_input=Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "embedding_right=embedding_layer(left_input)\n",
    "right_output=lstm_layer(embedding_right)\n",
    "\n",
    "combined_output=concatenate([left_output,right_output])\n",
    "combined_output = Dropout(rate_drop_dense)(combined_output)\n",
    "merged = BatchNormalization()(combined_output)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[left_input, right_input],outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STAMP = 'weights_preprocessing_lstm_glove_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727718 samples, validate on 80858 samples\n",
      "Epoch 1/20\n",
      "727718/727718 [==============================] - 1149s - loss: 0.5677 - acc: 0.7037 - val_loss: 0.5334 - val_acc: 0.7315\n",
      "Epoch 2/20\n",
      "727718/727718 [==============================] - 1124s - loss: 0.5299 - acc: 0.7339 - val_loss: 0.5172 - val_acc: 0.7436\n",
      "Epoch 3/20\n",
      "727718/727718 [==============================] - 1155s - loss: 0.5169 - acc: 0.7429 - val_loss: 0.5087 - val_acc: 0.7471\n",
      "Epoch 4/20\n",
      "727718/727718 [==============================] - 1133s - loss: 0.5092 - acc: 0.7482 - val_loss: 0.5040 - val_acc: 0.7529\n",
      "Epoch 5/20\n",
      "727718/727718 [==============================] - 1109s - loss: 0.5042 - acc: 0.7514 - val_loss: 0.5142 - val_acc: 0.7453\n",
      "Epoch 6/20\n",
      "727718/727718 [==============================] - 1108s - loss: 0.5002 - acc: 0.7546 - val_loss: 0.5009 - val_acc: 0.7540\n",
      "Epoch 7/20\n",
      "727718/727718 [==============================] - 1108s - loss: 0.4969 - acc: 0.7563 - val_loss: 0.5014 - val_acc: 0.7564\n",
      "Epoch 8/20\n",
      "727718/727718 [==============================] - 1109s - loss: 0.4951 - acc: 0.7574 - val_loss: 0.5034 - val_acc: 0.7539\n",
      "Epoch 9/20\n",
      "727718/727718 [==============================] - 1108s - loss: 0.4934 - acc: 0.7587 - val_loss: 0.4956 - val_acc: 0.7582\n",
      "Epoch 10/20\n",
      "727718/727718 [==============================] - 1108s - loss: 0.4916 - acc: 0.7599 - val_loss: 0.4977 - val_acc: 0.7580\n",
      "Epoch 11/20\n",
      "727718/727718 [==============================] - 1111s - loss: 0.4903 - acc: 0.7610 - val_loss: 0.4982 - val_acc: 0.7584\n",
      "Epoch 12/20\n",
      "727718/727718 [==============================] - 1112s - loss: 0.4897 - acc: 0.7614 - val_loss: 0.4969 - val_acc: 0.7586\n",
      "Epoch 13/20\n",
      "727718/727718 [==============================] - 1114s - loss: 0.4891 - acc: 0.7617 - val_loss: 0.4970 - val_acc: 0.7574\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val), \\\n",
    "        epochs=20, batch_size=200, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "2345796/2345796 [==============================] - 1521s  \n",
      "2345796/2345796 [==============================] - 1435s  \n"
     ]
    }
   ],
   "source": [
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2], batch_size=200, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=200, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
