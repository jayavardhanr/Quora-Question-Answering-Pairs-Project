{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Single model may achieve LB scores at around 0.29+ ~ 0.30+\n",
    "Average ensembles can easily get 0.28+ or less\n",
    "Don't need to be an expert of feature engineering\n",
    "All you need is a GPU!!!!!!!\n",
    "\n",
    "The code is tested on Keras 2.0.0 using Tensorflow backend, and Python 2.7\n",
    "\n",
    "According to experiments by kagglers, Theano backend with GPU may give bad LB scores while\n",
    "        the val_loss seems to be fine, so try Tensorflow backend first please\n",
    "'''\n",
    "\n",
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "WORDVECTORS_DIR='wordvectors/'\n",
    "INPUT_DATA_DIR='inputs/'\n",
    "WEIGHTS_DIR='weights/'\n",
    "OUTPUT_DIR='output/'\n",
    "\n",
    "EMBEDDING_FILE=WORDVECTORS_DIR+'glove.6B.100d.txt'\n",
    "TRAIN_DATA_FILE=INPUT_DATA_DIR+'train.csv'\n",
    "TEST_DATA_FILE=INPUT_DATA_DIR+'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 100\n",
    "num_dense = 50\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "#STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        #rate_drop_dense)\n",
    "\n",
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))  \n",
    "\n",
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n",
      "Found 2345796 texts in test.csv\n",
      "Found 120499 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n",
      "Preparing embedding matrix\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ddb0a0dd75c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0membedding_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Null word embeddings: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n",
    "\n",
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_weights = np.random.random((nb_words, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_weights[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_weights, axis=1) == 0))\n",
    "\n",
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STAMP = 'weights_preprocessing_lstm_glove_class_weighting_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 100)       12050000                                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           80400                                        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 200)           800                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            10050                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 50)            200                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             51                                           \n",
      "====================================================================================================\n",
      "Total params: 12,141,501.0\n",
      "Trainable params: 91,001.0\n",
      "Non-trainable params: 12,050,500.0\n",
      "____________________________________________________________________________________________________\n",
      "weights_preprocessing_lstm_glove_class_weighting_0.20_0.20\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_weights],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/20\n",
      "727722/727722 [==============================] - 1178s - loss: 0.3829 - acc: 0.6877 - val_loss: 0.3385 - val_acc: 0.7457\n",
      "Epoch 2/20\n",
      "727722/727722 [==============================] - 1258s - loss: 0.3407 - acc: 0.7242 - val_loss: 0.3134 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "727722/727722 [==============================] - 1288s - loss: 0.3296 - acc: 0.7353 - val_loss: 0.3067 - val_acc: 0.7571\n",
      "Epoch 4/20\n",
      "727722/727722 [==============================] - 1150s - loss: 0.3226 - acc: 0.7417 - val_loss: 0.3030 - val_acc: 0.7657\n",
      "Epoch 5/20\n",
      "727722/727722 [==============================] - 1110s - loss: 0.3184 - acc: 0.7460 - val_loss: 0.3006 - val_acc: 0.7673\n",
      "Epoch 6/20\n",
      "727722/727722 [==============================] - 1110s - loss: 0.3156 - acc: 0.7490 - val_loss: 0.3071 - val_acc: 0.7859\n",
      "Epoch 7/20\n",
      "727722/727722 [==============================] - 1117s - loss: 0.3133 - acc: 0.7520 - val_loss: 0.3022 - val_acc: 0.7820\n",
      "Epoch 8/20\n",
      "727722/727722 [==============================] - 1106s - loss: 0.3112 - acc: 0.7546 - val_loss: 0.2972 - val_acc: 0.7815\n",
      "Epoch 9/20\n",
      "727722/727722 [==============================] - 1102s - loss: 0.3095 - acc: 0.7560 - val_loss: 0.3031 - val_acc: 0.7890\n",
      "Epoch 10/20\n",
      "727722/727722 [==============================] - 1187s - loss: 0.3086 - acc: 0.7573 - val_loss: 0.2940 - val_acc: 0.7740\n",
      "Epoch 11/20\n",
      "727722/727722 [==============================] - 1266s - loss: 0.3078 - acc: 0.7574 - val_loss: 0.2934 - val_acc: 0.7808\n",
      "Epoch 12/20\n",
      "727722/727722 [==============================] - 1180s - loss: 0.3071 - acc: 0.7588 - val_loss: 0.2932 - val_acc: 0.7812\n",
      "Epoch 13/20\n",
      "727722/727722 [==============================] - 1175s - loss: 0.3065 - acc: 0.7595 - val_loss: 0.2943 - val_acc: 0.7877\n",
      "Epoch 14/20\n",
      "727722/727722 [==============================] - 1245s - loss: 0.3062 - acc: 0.7601 - val_loss: 0.2915 - val_acc: 0.7859\n",
      "Epoch 15/20\n",
      "727722/727722 [==============================] - 1252s - loss: 0.3049 - acc: 0.7610 - val_loss: 0.2952 - val_acc: 0.7910\n",
      "Epoch 16/20\n",
      "727722/727722 [==============================] - 1179s - loss: 0.3048 - acc: 0.7613 - val_loss: 0.2925 - val_acc: 0.7824\n",
      "Epoch 17/20\n",
      "727722/727722 [==============================] - 1199s - loss: 0.3045 - acc: 0.7614 - val_loss: 0.2902 - val_acc: 0.7846\n",
      "Epoch 18/20\n",
      "727722/727722 [==============================] - 1193s - loss: 0.3039 - acc: 0.7619 - val_loss: 0.2920 - val_acc: 0.7888\n",
      "Epoch 19/20\n",
      "727722/727722 [==============================] - 1237s - loss: 0.3037 - acc: 0.7625 - val_loss: 0.2864 - val_acc: 0.7831\n",
      "Epoch 20/20\n",
      "727722/727722 [==============================] - 1206s - loss: 0.3033 - acc: 0.7631 - val_loss: 0.2911 - val_acc: 0.7922\n"
     ]
    }
   ],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=20, batch_size=200, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28644573184258182"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "bst_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "2345796/2345796 [==============================] - 1804s  \n",
      "2345796/2345796 [==============================] - 1764s  \n"
     ]
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
